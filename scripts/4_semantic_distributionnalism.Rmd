---
title: "<img src=\"https://crctcs.openum.ca/files/sites/60/2024/03/cuso2.png\" style=\"float: left;margin-right:25px; margin-top:7.5px; height:150px\"/> Semantic Distributionalism"
author: "Maxime Sainte-Marie"
date: "`r format(Sys.time(), '%B %d %Y')`"
output:
  html_document:
    toc: yes
    toc_float: yes
    css: '../scripts/css_files/standard.css'
geometry: margin=1in
knit: (function(inputFile, encoding) {
  browseURL(
    rmarkdown::render(
      inputFile,
      encoding = encoding,
      output_dir = 'manuscripts',
      output_file = "semantic_distributionalism.html"))})

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Apparaîssant aux États-Unis vers 1930, le distributionnalisme connaît son aboutissement dans la publication par Zelig Harris du livre *Methods in Structural Linguistics* (1951). Au-delà des variantes théoriques et méthodologiques, le distributionnalisme propose une approche radicalement novatrice, fondée sur une conception à la fois behavioriste et statistique du langage. Dans un premier temps, le distributionnalisme postule que le langage peut s'expliquer par ses conditions externes d'apparition, de sorte qu'il est possible de rendre compte des comportements linguistiques sans aucune postulation concernant les intentions des locuteurs et leurs états mentaux. Selon cette perspective, le fonctionnement de la langue ne peut donc s'expliquer qu'inductivement, soit par l'analyse d'un corpus, c'est-à-dire un échantillon représentatif d'énoncés qui répond à la caractéristique d'être compris par les locuteurs de cette langue. Mais plus fondamentalement encore, le distributionnalisme considère le language comme un système de règles à plusieurs niveaux de contrainte, où chaque unité de la chaîne linguistique (phonologique, morphologique, phrastique) est définie par la distribution des combinaisons au niveau supérieur, telle que révélée par la distribution des éléments aux divers rangs. Par "distribution", Harris entend précisément ceci:

> "The distribution of an element is the total of all environments in which it occurs, i.e. the sum of all the (different) positions (or occurrences) of an element relative to the occurrence of other elements" (Harris, 1963, 15-16)

Opérationnellement, l'analyse de ces distributions permet l'identification de classes de phonèmes, morphèmes, syntagmes sur la base de leurs combinaisons et co-occurrences dans la successsion des élélments de la chaîne linguistique (classes syntagmatiques) ou selon leurs possibilités de commutation en un point de cette chaîne (classes paradigmatiques). En des termes formels plus modernes, le language est conçu comme un automate à état fini, le choix entre divers éléments en un point de la chaîne parlée étant variable, mais relativement déterminé par ce qui précède, à la manière des chaînes de Markov utilisées pour les suggestions de requête dans les moteurs de recherche de pages web. En mettant ainsi l'emphase sur les hiérarchies de distributions sous-jacentes aux productions linguistiques observées, le distributionnalisme encourage non seulement le développement de la linguistique de corpus, mais également l'utilisation de la statistique, de la théorie des probabilités ainsi que de la théorie de l'information dans l'étude du langage (Dubois, 1969).

C'est toutefois les contributions du distributionnalisme à la sémantique, c'est-à-dire à l'étude des structures, mécanismes et phénomènes linguistiques de construction et d'attribution, qui nous intéressent particulièrement ici. Sur ce point, bien que les idées de Harris en matière de sémantique font encore l'objet de nombreuses controverses, il semble toutefois possible d'affirmer que l'auteur prône une conception "faible" de la sémantique, davantage différentielle que référentielle, basée sur l'existence d'une correspondence entre les différences sémantiques et distributionnelles

> "The fact that, for example, not every adjective occurs with every noun can be used as a measure of meaning difference. For it is not merely that different members of one class have different selections of members of the other class with which they are actually found. More than that: if we consider words or morphemes A or B to be more different than A and C, then we will often find that the distributions of A and B are more different than the distributions of A and C. In other words, difference in meaning correlates with difference in distribution" (Harris, 1954, 43)

À la même époque, plusieurs chercheurs provenant de disciplines et pays divers mettent également l'emphase sur l'importance sémantique des contextes de co-occurrence. Du côté de l'Angleterre, les écrits et cours de Ludwig Wittgenstein, philosophe autricien et proefesseur à l'Université de Cambridge, jouent un rôle précurseur dans cette entreprise, surtout en ce qui a trait à ses considérations sur les jeux de langage et les ressemblances de famille ainsi qu'à sa définition de la signification par l'usage (Wittgenstein 2014, 43). Un pragmatisme sémantique similaire se retrouve également dans les recherches sur les collocations du linguiste anglais John Rupert Firth, pour qui un mot se reconnaît par les autres mots qui l'entourent ("you shall know a word by the company it keeps" (Firth, 1957, 11)). Toujours à la même époque, le mathématicien Warrent Weaver, co-créateur avec Claude Shannon de l'article fondateur de la théorie de l'information, affirme dans l'un des articles-phares de la traduction automatique que la désambigüisation sémantique des mots à traduire devrait être basée sur la fréquence de co-occurrence des contextes lexicaux dans lesquels ce mot apparaît (Weaver 1955). C'est d'ailleurs dans le domaine de la traduction automatique que le terme "sémantique distributionnelle" est utilisé pour l'une des premières fois, soit en référence à un programme de recherche inspiré du distributionnalisme de Harris (Garvin 1962). Du côté des sciences cognitives, l'un des plus ardents défenseurs de l'importance sémantique des distributions lexicales est le linguiste américain George Miller (1967), pour qui l'analyse distributionnelle de Harris est à même de fournir une base empirique au concept de similarité sémantique.

Derrière ces différentes considérations sur la nature distribuée de la signification lexicale se cache en vérité une panoplie d'interprétations et de définitions. Bien que l'inventaire de ces conceptions dépasse le cadre du présent cours, il semble toutefois pertinent de souligner ici que la portée épistémologique et les applications de l'hypothèse distributionnelle peuvent varier selon que l'on considère la dépendance fonctionnelle entre distributions lexicales et relations sémantiques comme une relation corrélationnelle ou causale. Dans cette perspective, Lenci (2008) propose de distinguer deux variantes de l'hypothèse distributionnelle, une forte et une faible.

Dans sa version plus forte et d'interprétation causale, l'hypothèse distributionnelle poste que les distributions lexicales ont un rôle déterminant sur l'origine, le développement et la forme cognitives de nos représentations sémantiques

> "The cognitive representation of a word is some abstraction or generalization derived from the contexts that have been encountered. That is to say, a word's contextual representation is not itself a linguistic context, but is an abstract cognitive structure that accumulates from encounters with the word in various (linguistic) contexts" (Miller and Charles, 1991, p. 5)

De telles représentations sémantiques distributionnelles ont servi à la modélisation de différentes tâches de traitement sémantique, notamment au niveau des jugements de similarité (Miller and Charles, 1991; Rubinstein and Goodenough, 1965), de l'amorçage associatif et sémantique (Jones, Kintsch, and Mewhort, 2006; Lund et Burgess 1996), des mémoires sémantique (Rogers et al. 2004; McRae et Jones 2013, Jones et al. 2015) et épisodique (Griffiths et al. 2007) ainsi que de la compréhension de texte en général (Landauer et Dumais 1997) comme du développement lexical et grammatical chez l'enfant (Borovski et Elman 2006; Li, Farkas, and MacWhinney, 2004; Baroni, Lenci et Onnis, 2007). D'ailleurs, certains des modèles et algorithmes de sémantique distributionnelle les plus populaires, par exemple l'analyse sémantique latente (*Latent Semantic Analysis*) (Landauer et Dumais 1997; Landauer et al. 2013) et l'hyperespace analogue à la langue (*Hyperspace Analogue to Language*) (Burgess and Lund 1997; Farkas and Li 2001), ont précisément été conçus en vue de modéliser de manière cognitivement plausible l'apprentissage des représentations sémantiques par extraction de patrons de co-occurrences linguistiques.

Plus proche de la conception originale de Harris et des objectifs du présent cours, la version faible et corrélationnelle de l'hypothèse distributionnelle est cognitivement neutre: elle ne fournit aucune définition de la signification et ne tente pas d'expliquer le rapport entre distributions lexicales et relations sémantiques. Tout ce qu'elle présume, c'est l'existence d'une corrélation entre celles-ci, corrélation pouvant être opérationnalisée à des fins d'analyse sémantique

> "The idea is that word meaning (whatever this might be) determines the combinatorial behavior of words in contexts, and the semantic features of lexical expressions act as contraints governing their syntagmatic behavior. Consequently, by inspecting a relevant number of distributional contexts, we may hope to be able to identify those aspects of meanings that are shared by words with similar contextual distributions and that can be used to explain these very distributions. (...) Assuming this weak version of the DH does not entail assuming that word distributions are themselves constitutive of the semantic properties of lexical items at a cognitive level. It rather corresponds to taking semantics as a kind of "latent varible" which is responsible for the linguistic distributions that we observe, and that we try to uncover by inspecting a significant number of such distributions" (Lenci 2008, 4)

Au-delà de cette opposition entre interprétations forte et faible fondée sur le contraste conceptuel entre corrélation et causalité, la sémantique distributionnelle repose toutefois de nos jours sur un cadre méthodologique et implémentationnel commun, fondé sur l'algèbre vectorielle ainsi que sur l'hypothèse suivante, visant à fournir un cadre opérationnel général à l'analyse des mécanismes et phénomènes d'attribution de sens basée sur l'étude des distributions lexicales.

> **Statistical semantic hypothesis**: If units of text have similar vectors in a text frequency matrix, then they tend to have similar meanings" (Turney and Pantel, 2010, p. 153)

Des premières ébauches d'opérationnalisation du distributionnalisme sémantique ont été proposées pour la construction automatique de thésaurus dès les années 60 (Grefenstette 2012). C'est toutefois dans le domaine de la recherche d'information qu'est développé le premier véritable modèle d'analyse sémantique basé sur la modélisation vectorielle de l'usage lexical et des contextes de co-occurrence. Développé par Gerard Salton et ses collègues de l'université Cornell, le système SMART (System for the Mechanical Analysis and Retrieval of Text) (Salton, Wong, and Yant 1975) est un système d'extraction d'information sémantique dont les principes et méthodes ont servi de base aux engins de recherche actuels. À la base, le fonctionnement de SMART repose sur la modélisation vectorielle d'une collection de documents à partir des fréquences d'occurrence des types lexicaux contenus dans chaque document. À proprement parler, le système SMART n'est pas le premier modèle vectoriel de modélisation sémantique. En 1952, le psychologue américains Charles Egerton Osgood, créateur de l'échelle sémantique différentielle, publiait *The Nature and Measurement of Meaning*, article dans lequel il définissait un système sémantique comme un espace de vecteurs de concepts à n dimensions (Osgood 1952). L'originalité du système SMART repose sur la vectorisation des fréquences lexicales des documents d'un corpus à des fins d'analyse sémantique (Turney and Pantel 2010); ce processus de modélisation consiste à créer une matrice ayant pour colonnes les différents documents de la collection, pour rangées les différents types lexicaux de la collection et dans laquelle la valeur de chaque cellule est déterminée par le nombre d'occurrences du type lexical de la rangée en question au sein du document de la colonne correspondante. De cette manière, chaque document est converti en vecteur, puis projeté dans l'espace vectoriel tel un point, de telle manière que la distance cosinus entre chaque paire de documents-points est inversement proportionnelle à leur similarité sémantique. La requête de l'utilisateur est alors convertie en pseudo-document, puis ajoutée à l'espace vectoriel des documents, après quoi les documents de l'espace sont ordonnées par ordre croissant de distance cosinus (ordre décroissant de similarité sémantique) par rapport à la requête vectorisée, puis présentés à l'utilisateur.

Si le modèle sémantique vectoriel a continué d'être utilisé en recherche d'information, son utilisation dans d'autres tâches de traitement automatisé du langage naturel n'a véritablement pris son envol que dans les années 90, probablement en raison de la place prépondérante accordée jusqu'alors en linguistique computationnelle aux modèles formels et logiques du language (Lenci 2018). Toutefois, les développements fulgurants des technologies de l'information à cette époque, conjointement à l'accroissement en disponibilité et taille des corpus électroniques, a grandement contribué au développement et à la popularisation des modèles sémantiques distributionnels. Également appelés espaces sémantiques ou de mots (word space models) (Schütze 1993), modèles sémantiques vectoriels ou géométriques (Widdows 2004), modèles contextuels (context-theoretic) ou basés corpus (corpus-based), ces modèles ont depuis démontré leur utilié et performance à de nombreuses reprises, notamment en comparaison à l'utilisation de thésaurus spécialisés (Grefenstette 1982), de réseaux sémantiques (Curran et Moens, 2002; Padró et al. 2014; Henestroza Anguiano et Denis 2011), de dictionnaires de synonymes (Plas, Tiedemann and Manguin 2011) et de nombreux jeux de données de référence, comme par exemple les résultats de participants à différentes tâches linguistiques (Landauer et Dumais 1997). À ce titre d'ailleurs, un modèle vectoriel du British National Corpus a été développé par Rapp (2004) afin de répondre au Test of English as Foreign Language (TOEFL); ce modèle a obtenu la note de 92.5%, résultat de loin supérieur à la performance humaine moyenne de 64.5%. Dans le même ordre d'idées, Turney (2006) a utilisé un modèle vectoriel de représentation des relations sémantiques pour obtenir un score de 56% sur un sous-ensemble de questions à choix multiples tirée des tests américains d'entrée à l'université (SAT College Entrance Tests), la moyenne humaine pour ce sous-ensemble de questions étant de 57%. Par ailleurs, les modèles sémantiques distributionnels ont également démontré leur utilité et performance dans des tâches linguistiques complexes comme le traitement de requêtes en recherche d'information (Alfonseca, Hall, and Hartmann 2009; Claveau et Kijak 2015), la substitution lexicale (Fabre et al. 2014; McCarthy et Navigli 2009), la désambiguïsation lexicale par regroupement statistique d'usages (McCarthy et al. 2007), l'implication textuelle et le résumé automatique de textes (Cheung and Penn 2013) ainsi que l'étiquetage sémantique et la reconnaissance d'entités nommées (Collobert and Weston 2008).

L'induction de sens (Schütze 1998), l'apprentissage ontologique (Hindle 1990; Lin 1998; Ravichandran et al. 2005; Gorman and Curran 2006; Snow et al. 2006), predominant word sense determination (McCarthy et al. 2004; Padó and Lapata 2007), predicting similarity of semantic relations (Turney 2006), and inference rule learning (Lin and Pantel 2001).

Several authors have criticized VSMs (French and Labiouse, 2002; Padó et Lapata 2003; Morris et Hirst 2004; Budanitsky and Hirst 2006). Most of the criticism stems from the fact that these models ignore word order and thus does not represent relational information for example; for example, 'boat house' and 'house boat' will be represented by the same vector. However, Landauer estimates that 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order. Additionally, many modelling sophistications of the standard SDM have been designed in order to account for word order.

Also, there is no way to represent arbitrary statements in first-order predicate calculus. Several aspects of meaning (quantification, intensionality, negation) cannot be represented in such a way

The limits of the distributional semantics may arise from two factors: Some semantic facts might not be handled in terms of nonsymbolic representations, and/or they might not have a correlate in distributional statistics harvested from corpus data (Lenci 2018).

Actuellement, les tendances sont à l'utilisation de corpus les plus volumineux possibles, avec un nombre de mots dépassant souvent le milliard. Les petits corpus traités avec des méthodes distributionnelles sont généralement de l'ordre de la centaine de millions de mots. Or, en langue de spécialité, les corpus sont généralement de plus petite taille, plutôt que de l'ordre du million de mots.

En sommes, les modèles sémantiques distributionnels gagnent en popularité et en crédibilité, notamment à des fins de modélisation et de recherche linguistique et cognitive. Le nombre de tâches auxquelles ces systèmes sont soumis est en croissance constante et va désormais bien au-delà de l'hypothèse distributionnelle à des fins d'identification de synonymes